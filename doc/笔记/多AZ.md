### 一、基本架构：多 AZ 是什么？

- **AZ（Availability Zone，可用区）**：指在同一地域（Region）内，电力、网络、物理位置相互隔离的独立数据中心。
- **多 AZ 存储**：将数据跨多个 AZ 分布存储，避免单点故障。

例如：某云厂商在“上海”地域有 3 个 AZ（AZ-A、AZ-B、AZ-C），多 AZ 存储会把用户的数据分散存储在这 3 个独立的数据中心中。

---

### 二、关键技术：纠删码（Erasure Coding, EC）

这是实现高可靠、低成本冗余的核心技术。

#### 1. 纠删码原理

假设使用 **EC(6,3)** 算法：

- 原始数据被切分为 6 个数据块；
- 通过算法计算出 3 个校验块；
- 总共 9 个块，分别存储在 **不同 AZ 的不同设备上**；
- 只要任意 **6 个块**（数据块或校验块）可用，就能恢复出原始数据。

> 也就是说，最多允许 **3 个块丢失或不可访问**，数据依然可恢复。

#### 2. 数据分布策略

- 每个数据块和校验块被分散存储在不同 AZ 中（如每个 AZ 存 3 个块）；
- 通常会避免将同一对象的所有块放在同一个 AZ 或同一台设备上；
- 实现“跨 AZ 冗余”。

---

### 三、如何实现“故障时数据不丢、业务不受影响”？

#### 场景：某个数据中心（如 AZ-A）整体宕机（断电、网络中断等）

##### 1. 数据不丢失 ✅

- 虽然 AZ-A 中的 3 个数据/校验块无法访问；
- 但其余 6 个块（分布在 AZ-B 和 AZ-C）仍然可读；
- 根据纠删码算法，只要有任意 6 个块，就能完整恢复原始数据；
- → 数据**逻辑上依然完整**，未丢失。

##### 2. 读写服务不受影响 ✅

- **读操作**：
    - 系统从仍可用的 AZ（如 AZ-B、AZ-C）中读取所需的 6 个块；
    - 在内存中重建原始数据并返回给用户；
    - 用户无感知，读取正常。

- **写操作**：
    - 新数据写入时，系统将数据切分 + 编码为 9 个块；
    - 只将块写入 **仍可用的 AZ（AZ-B、AZ-C）** 中的节点；
    - 等 AZ-A 恢复后，系统自动同步补全缺失块；
    - 写入过程**无需等待故障 AZ 恢复**，持续可用。

##### 3. 自动故障转移与恢复

- 存储系统具备全局元数据管理和服务路由能力；
- 当检测到某 AZ 不可达时，自动将流量切换到健康 AZ；
- 后台启动“数据再平衡”或“修复进程”，在 AZ 恢复后自动补齐冗余。

---

### 四、为什么可靠性高达 99.9999999999%（12 个 9）？

这是通过**数学建模 + 多重冗余 + 故障域隔离**计算得出的：

- 单盘年故障率约 0.5%~1%；
- 单数据中心故障概率较低，但依然存在（如火灾、断电）；
- 多 AZ + 纠删码使系统能容忍 **多个设备甚至整个 AZ 故障**；
- 多重保护机制（如跨机架、跨电源、跨网络）降低相关性故障；
- 综合计算后，数据**年丢失概率低于 0.0000000001%**，即 12 个 9。

---

### 五、服务可用性 99.995% 是如何实现的？

- 即全年不可用时间 ≤ **26.3 分钟**；
- 通过：
    - 多 AZ 架构避免单点中断；
    - 自动故障转移（failover）无需人工干预；
    - 并发读写、负载均衡、快速重试机制；
    - 高可用元数据服务（如分布式共识算法 Raft/Paxos）；
- 即使部分节点或 AZ 故障，前端服务仍可响应请求。

---

### 六、用户视角：便捷易用

- 用户无需关心底层数据分布；
- 只需在创建对象或存储桶时，选择“多 AZ 存储类型”；
- 系统自动完成：
    - 数据分块；
    - 纠删码计算；
    - 跨 AZ 分发存储；
    - 故障恢复；
- 读写接口与普通存储一致，完全透明。

---

### 总结：如何做到“数据不丢、业务不停”？

| 目标         | 实现方式                                   |
|------------|----------------------------------------|
| **数据不丢失**  | 使用纠删码（如 EC(6,3)），允许丢失部分块仍可恢复；数据跨 AZ 存储 |
| **业务不受影响** | 故障 AZ 被自动绕过，读写在健康 AZ 上完成；系统自动修复        |
| **高可用性**   | 多 AZ 隔离 + 自动故障转移 + 高可用控制平面             |
| **高持久性**   | 12 个 9 的可靠性设计，远超传统 RAID 或副本机制          |

