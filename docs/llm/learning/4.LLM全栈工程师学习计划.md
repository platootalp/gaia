# 🚀 综合LLM应用开发学习计划（150天·分层技术栈版）

> **作者：** 哦，我的朋友
> **目标：** 从 Java 后端工程师 → 全栈 LLM 应用开发工程师  
> **周期：** 150天（约5个月）  
> **时间规划：** 每天 2~3 小时（可灵活调整）  
> **技术分层原则：**
> - **Python 技术栈**：大模型核心（推理、RAG、Agent、微调）
> - **Java 技术栈**：工程化部分（API 网关、任务调度、监控、用户系统）
> - **TypeScript 技术栈**：前端交互（流式 UI、状态管理）  
    > **系统集成：** 通过 REST/gRPC/消息队列松耦合连接三层架构

---

## 🌟 学习计划总览

| 阶段   | 时间范围      | 学习主题      | 核心技能                                      | 阶段目标                     |
|------|-----------|-----------|-------------------------------------------|--------------------------|
| 第1阶段 | D1-D25    | 基础与集成     | OpenAI API、Prompt工程、Spring Boot调用、基础聊天UI  | 掌握LLM基础调用与三层架构初步集成       |
| 第2阶段 | D26-D45   | 本地推理服务    | vLLM部署、FastAPI封装、Resilience4j熔断、SSE流式响应   | 能独立部署本地LLM推理服务并集成到Java系统 |
| 第3阶段 | D46-D70   | RAG知识增强系统 | 文档解析、Qdrant向量库、RAG-Fusion、Ragas评估、文件管理    | 构建企业级多模态文档问答系统           |
| 第4阶段 | D71-D105  | 智能体与工作流   | LangGraph状态机、工具调用、多Agent协作、任务状态管理、Webhook | 实现可审计、可追溯的智能体工作流系统       |
| 第5阶段 | D106-D140 | 优化与微调     | QLoRA微调、AWQ量化、Token监控、缓存策略、Grafana监控面板    | 掌握高效推理与定制化模型能力           |
| 第6阶段 | D141-D150 | 全链路产品化    | K8s部署、Helm Chart、CI/CD流水线、三端协同            | 完成生产级LLM应用系统交付           |

---

## 🧱 第1阶段：基础与集成（Day 1-25）

### 🎯 阶段目标

- 掌握主流LLM API调用与Prompt工程技巧
- 实现Spring Boot服务调用Python LLM接口
- 构建基础聊天Web界面
- 建立三层架构初步集成

### 📚 学习内容

#### 1.1 Python基础LLM调用（D1-D8）

- **OpenAI API**：Chat Completions接口、流式响应、函数调用
- **Ollama本地模型**：`llama3`、`mistral`安装与调用
- **Prompt工程**：Few-Shot示例、思维链(CoT)、JSON Schema结构化输出
- **错误处理**：重试机制、速率限制应对、异常捕获

**实践任务：**

- 用Python实现OpenAI聊天接口（带历史记录）
- 对比GPT-4o与Llama3在相同任务上的表现
- 设计商品描述生成的Prompt模板（含Few-Shot示例）

#### 1.2 Python服务封装（D9-D15）

- **FastAPI基础**：路由定义、请求验证、依赖注入
- **OpenAPI兼容接口**：实现`/v1/chat/completions`
- **中间件**：请求日志、Token统计、限流
- **Docker基础**：构建Python服务镜像

**实践任务：**

- 用FastAPI封装LLM聊天接口
- 实现基于IP的速率限制（每分钟10请求）
- 编写Dockerfile容器化服务

#### 1.3 Java服务集成（D16-D21）

- **Spring Boot项目**：创建`llm-backend`服务
- **WebClient调用**：异步调用Python LLM服务
- **熔断设计**：Resilience4j实现服务降级
- **统一响应**：标准化API响应格式

**实践任务：**

- 实现LLM服务代理（支持OpenAI/Ollama双后端）
- 配置熔断：连续3次失败切换备用服务
- 添加请求ID透传（用于链路追踪）

#### 1.4 TypeScript基础前端（D22-D25）

- **Next.js基础**：页面路由、组件化
- **API调用**：`useEffect` + `fetch`
- **聊天界面**：消息气泡、输入框、滚动到底部
- **错误处理**：网络错误、服务不可用提示

**实践任务：**

- 构建基础聊天界面（支持发送/接收）
- 实现消息历史记录本地存储
- 添加加载状态和错误提示

### 📖 推荐资源

| 类型  | 资源                                                                | 说明           |
|-----|-------------------------------------------------------------------|--------------|
| API | [OpenAI API文档](https://platform.openai.com/docs/api-reference)    | 官方API参考      |
| 框架  | [FastAPI教程](https://fastapi.tiangolo.com/tutorial/)               | Python API开发 |
| 工具  | [Ollama](https://ollama.com/)                                     | 本地模型运行       |
| 框架  | [Spring WebClient指南](https://www.baeldung.com/spring-5-webclient) | 异步HTTP客户端    |
| 框架  | [Next.js学习](https://nextjs.org/learn)                             | React框架      |

### 🎯 阶段成果

- **Python服务**：支持多模型的聊天API（FastAPI + Docker）
- **Java服务**：带熔断的LLM代理服务（Spring Boot）
- **前端应用**：基础聊天界面（Next.js）
- **集成Demo**：用户输入 → Next.js → Spring Boot → FastAPI → LLM → 返回结果

---

## ⚙️ 第2阶段：本地推理服务（Day 26-45）

### 🎯 阶段目标

- 掌握vLLM高性能推理引擎部署
- 实现Java服务对推理服务的负载均衡与熔断
- 构建前端流式响应体验
- 完成服务容器化与基础监控

### 📚 学习内容

#### 2.1 vLLM高性能推理（D26-D32）

- **vLLM安装**：CUDA 12.1 + PyTorch 2.3环境
- **模型加载**：Llama 3 8B、Mistral 7B
- **PagedAttention**：显存优化技术原理
- **连续批处理**：动态合并请求提升吞吐

**实践任务：**

- 在GPU服务器部署vLLM + Llama 3 8B
- 压测不同batch size下的吞吐量（requests/sec）
- 对比vLLM与Ollama在相同硬件上的性能

#### 2.2 服务增强与安全（D33-D38）

- **认证机制**：API Key验证（FastAPI依赖项）
- **输入过滤**：敏感内容检测、长度限制
- **速率限制**：基于Redis的分布式限流
- **Prometheus指标**：暴露QPS、延迟、错误率

**实践任务：**

- 实现API Key验证中间件
- 添加输入长度限制（max_tokens=8192）
- 集成Prometheus客户端库

#### 2.3 Java服务增强（D39-D42）

- **服务发现**：Python服务注册到Consul
- **负载均衡**：Spring Cloud LoadBalancer
- **重试策略**：指数退避算法
- **指标集成**：Micrometer上报LLM调用指标

**实践任务：**

- 配置Consul服务注册与发现
- 实现基于响应时间的负载均衡策略
- 添加Micrometer指标（llm_request_duration_seconds）

#### 2.4 流式前端体验（D43-D45）

- **SSE协议**：Server-Sent Events
- **打字机效果**：逐字显示、光标闪烁
- **取消请求**：AbortController
- **消息分块**：处理流式JSON

**实践任务：**

- 实现SSE客户端连接
- 添加打字机动画效果
- 支持取消长生成请求

### 📖 推荐资源

| 类型 | 资源                                                                               | 说明    |
|----|----------------------------------------------------------------------------------|-------|
| 引擎 | [vLLM文档](https://docs.vllm.ai/)                                                  | 高性能推理 |
| 监控 | [Prometheus Client for Python](https://github.com/prometheus/client_python)      | 指标暴露  |
| 服务 | [Spring Cloud Consul](https://spring.io/projects/spring-cloud-consul)            | 服务发现  |
| 前端 | [MDN SSE指南](https://developer.mozilla.org/zh-CN/docs/Web/API/Server-sent_events) | 流式通信  |

### 🎯 阶段成果

- **推理服务**：vLLM + Llama 3 8B（Docker部署，P99延迟<1s）
- **Java网关**：带服务发现、负载均衡、熔断的API网关
- **前端体验**：支持流式响应、取消请求的聊天界面
- **监控指标**：基础Prometheus指标（QPS、延迟、错误率）

---

## 🔍 第3阶段：RAG知识增强系统（Day 46-70）

### 🎯 阶段目标

- 构建支持多格式文档的解析管道
- 实现高精度向量检索与重排序
- 集成Ragas评估框架
- 完成Java层文件管理与权限控制

### 📚 学习内容

#### 3.1 文档处理管道（D46-D52）

- **文档加载**：Unstructured库（PDF/Word/PPT）
- **文本分片**：语义分块（RecursiveCharacterTextSplitter）
- **元数据增强**：自动提取标题、章节、页码
- **多模态处理**：MinerU提取表格/公式，Qwen-VL理解图像

**实践任务：**

- 构建FastAPI文档上传接口
- 实现PDF表格精准提取
- 添加图像描述生成功能（Qwen-VL）

#### 3.2 向量检索优化（D53-D59）

- **Embedding模型**：text-embedding-3-large, bge-large-en-v1.5
- **Qdrant向量库**：Docker部署、集合创建、索引优化
- **混合检索**：BM25关键词 + 向量语义检索
- **重排序**：Cohere Rerank或bge-reranker-large

**实践任务：**

- 部署Qdrant（持久化存储）
- 实现混合检索（关键词+向量）
- 集成重排序模型提升结果相关性

#### 3.3 RAG-Fusion优化（D60-D65）

- **查询扩展**：生成3-5个相关查询
- **RRF融合**：Reciprocal Rank Fusion算法
- **上下文压缩**：仅保留相关句子
- **Ragas评估**：context_recall, faithfulness, answer_relevancy

**实践任务：**

- 实现RAG-Fusion检索链
- 添加上下文压缩中间件
- 生成Ragas评估报告（100个测试问题）

#### 3.4 Java服务集成（D66-D70）

- **文件管理**：Spring Boot文件上传API
- **异步处理**：RabbitMQ任务队列
- **权限控制**：JWT鉴权 + 文档归属
- **任务状态**：WebSocket实时进度通知

**实践任务：**

- 实现多文件上传接口（支持拖拽）
- 配置RabbitMQ消息队列处理文档解析
- 添加文档权限管理（用户/团队/公开）

### 📖 推荐资源

| 类型  | 资源                                                                      | 说明    |
|-----|-------------------------------------------------------------------------|-------|
| 工具  | [Unstructured](https://github.com/Unstructured-IO/unstructured)         | 文档解析  |
| 数据库 | [Qdrant](https://qdrant.tech/)                                          | 向量数据库 |
| 评估  | [Ragas](https://docs.ragas.io/)                                         | RAG评估 |
| 框架  | [RabbitMQ Spring Boot](https://spring.io/guides/gs/messaging-rabbitmq/) | 消息队列  |

### 🎯 阶段成果

- **RAG系统**：支持PDF/Word/图像的多模态知识库
- **评估报告**：Ragas指标对比（基础RAG vs RAG-Fusion）
- **文件管理**：带权限控制的文档管理系统
- **用户界面**：文档上传+知识问答一体化界面

---

## 🤖 第4阶段：智能体与工作流（Day 71-105）

### 🎯 阶段目标

- 掌握LangGraph状态机编程模型
- 实现多工具调用与人工审核节点
- 构建任务状态管理与通知系统
- 完成可审计、可追溯的工作流

### 📚 学习内容

#### 4.1 LangGraph核心（D71-D78）

- **状态机设计**：State定义、Node（动作）、Edge（条件跳转）
- **检查点机制**：自动保存/恢复对话状态
- **条件路由**：基于内容动态选择下一步
- **循环控制**：最大迭代次数、人工介入阈值

**实践任务：**

- 实现"研究问题→搜索→总结"工作流
- 配置Redis状态持久化
- 添加最大步骤限制（防止无限循环）

#### 4.2 工具开发（D79-D85）

- **工具注册**：@tool装饰器 + JSON Schema
- **自定义工具**：天气查询、计算器、数据库查询
- **安全控制**：输入验证、权限检查
- **工具组合**：多工具协同完成复杂任务

**实践任务：**

- 开发"查天气"、"算数学题"工具
- 实现安全SQL查询工具（参数化查询）
- 构建"旅行规划"工具链（航班+酒店+预算）

#### 4.3 多Agent协作（D86-D93）

- **Planner Agent**：任务分解（Qwen-Max）
- **Executor Agent**：子任务执行（Qwen-Plus）
- **Reviewer Agent**：结果验证
- **人工审核节点**：高风险操作暂停

**实践任务：**

- 构建信贷审批Agent（自动收集材料→风险评估→人工审核）
- 实现多Agent成本优化（复杂任务用大模型，简单任务用小模型）
- 添加人工审核界面（暂停/继续/拒绝）

#### 4.4 Java系统集成（D94-D105）

- **任务提交API**：/agent-tasks（JSON Schema验证）
- **状态管理**：PostgreSQL存储任务状态
- **Webhook通知**：完成时回调指定URL
- **审计日志**：记录完整操作链（谁/何时/做了什么）

**实践任务：**

- 实现任务状态查询接口（/tasks/{id}）
- 配置Webhook通知（成功/失败回调）
- 构建审计日志系统（操作记录+快照）

### 📖 推荐资源

| 类型  | 资源                                                                                                       | 说明      |
|-----|----------------------------------------------------------------------------------------------------------|---------|
| 框架  | [LangGraph文档](https://langchain-ai.github.io/langgraph/)                                                 | 状态机开发   |
| 监控  | [LangSmith](https://smith.langchain.com/)                                                                | Agent调试 |
| 数据库 | [Spring Data JPA](https://spring.io/projects/spring-data-jpa)                                            | 状态持久化   |
| 协议  | [Webhook签名验证](https://docs.github.com/en/developers/webhooks-and-events/webhooks/securing-your-webhooks) | 安全回调    |

### 🎯 阶段成果

- **智能体系统**：带人工审核的信贷审批Agent
- **任务管理**：完整任务生命周期API（提交→执行→通知→查询）
- **审计追踪**：操作日志与状态快照
- **前端界面**：任务提交表单+状态看板+审核界面

---

## ⚡ 第5阶段：优化与微调（Day 106-140）

### 🎯 阶段目标

- 掌握高效模型微调（QLoRA）与量化（AWQ）
- 实现全链路性能监控与成本控制
- 构建缓存策略与降级机制
- 完成生产级可观测体系

### 📚 学习内容

#### 5.1 QLoRA微调（D106-D115）

- **Unsloth加速**：LoRA训练2x速度
- **数据准备**：Alpaca格式（instruction, input, output）
- **领域适应**：金融/医疗/法律特定数据集
- **评估指标**：loss曲线、生成质量、领域术语准确率

**实践任务：**

- 在单卡24G GPU上微调Llama 3 8B
- 用自定义数据集增强金融术语理解
- 生成微调前后对比报告

#### 5.2 模型量化与部署（D116-D125）

- **AWQ/GPTQ**：4-bit量化技术
- **vLLM + AWQ**：部署量化模型
- **内存优化**：PagedAttention + KV Cache管理
- **批处理调优**：动态batch size调整

**实践任务：**

- 量化Llama 3 8B到4-bit
- 部署到vLLM并压测性能
- 对比FP16与INT4的延迟/显存/质量

#### 5.3 全链路监控（D126-D132）

- **Prometheus指标**：
    - llm_requests_total
    - llm_latency_seconds
    - token_usage_total
- **Grafana面板**：QPS、P99延迟、错误率、成本
- **告警规则**：延迟突增、错误率超标、成本阈值
- **链路追踪**：OpenTelemetry分布式追踪

**实践任务：**

- 配置全链路指标收集（前端→Java→Python）
- 创建Grafana监控大屏
- 设置成本告警（每日Token消耗>100万）

#### 5.4 优化策略（D133-D140）

- **缓存机制**：Redis缓存相同Prompt结果
- **降级策略**：高负载时切换小模型
- **熔断机制**：错误率>5%时暂停服务
- **预热策略**：高峰前提前加载模型

**实践任务：**

- 实现基于SHA256(prompt)的缓存
- 配置自动降级规则（QPS>100时切换Qwen-Turbo）
- 设计熔断恢复流程（自动探测+人工确认）

### 📖 推荐资源

| 类型 | 资源                                                                        | 说明     |
|----|---------------------------------------------------------------------------|--------|
| 微调 | [Unsloth](https://github.com/unslothai/unsloth)                           | 加速LoRA |
| 量化 | [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)                       | 量化工具   |
| 监控 | [Prometheus + Grafana](https://prometheus.io/docs/visualization/grafana/) | 监控体系   |
| 链路 | [OpenTelemetry](https://opentelemetry.io/)                                | 分布式追踪  |

### 🎯 阶段成果

- **优化模型**：微调+量化后的领域专用模型（INT4）
- **监控体系**：全链路Grafana监控大屏
- **成本报告**：优化前后Token消耗对比
- **降级方案**：完整的故障应对流程文档

---

## 🌐 第6阶段：全链路产品化（Day 141-150）

### 🎯 阶段目标

- 完成Kubernetes生产部署
- 建立CI/CD自动化流水线
- 构建完整的用户管理与计费系统
- 交付可演示的产品级应用

### 📚 学习内容

#### 6.1 K8s部署架构（D141-D143）

- **Helm Chart**：定义三端部署模板
    - Java: Spring Boot Deployment
    - Python: vLLM + FastAPI（GPU节点）
    - Frontend: Next.js静态资源
- **资源分配**：
    - GPU节点：nvidia.com/gpu: 1
    - CPU节点：limits: 2, requests: 1
- **网络配置**：Ingress路由、Service Mesh

**实践任务：**

- 编写Helm Chart（values.yaml, templates）
- 配置GPU节点亲和性
- 设置健康检查（/health, /ready）

#### 6.2 CI/CD流水线（D144-D146）

- **GitHub Actions**：
    - Python: 测试 → 构建vLLM镜像 → 推送
    - Java: 单元测试 → 构建JAR → Docker镜像
    - TS: lint → build → 部署到Vercel
- **安全扫描**：Trivy镜像扫描、OWASP依赖检查
- **环境管理**：dev/staging/prod多环境

**实践任务：**

- 配置PR触发测试
- 实现main分支自动部署到staging
- 添加安全扫描步骤

#### 6.3 产品化功能（D147-D148）

- **用户系统**：注册/登录/团队管理（Spring Security）
- **计费系统**：Token用量统计、套餐管理
- **API Key管理**：生成/轮换/权限控制
- **数据分析**：用户行为分析、热门查询

**实践任务：**

- 实现基于Token用量的计费
- 构建API Key管理系统
- 添加用户活动仪表盘

#### 6.4 产品交付（D149-D150）

- **文档编写**：
    - 架构设计文档
    - API文档（Swagger）
    - 部署指南
- **演示视频**：3分钟功能演示
- **压力测试**：Locust模拟1000并发

**实践任务：**

- 生成完整技术文档
- 录制产品演示视频
- 编写压力测试报告

### 📖 推荐资源

| 类型    | 资源                                                     | 说明     |
|-------|--------------------------------------------------------|--------|
| 部署    | [Helm官方文档](https://helm.sh/docs/)                      | K8s包管理 |
| CI/CD | [GitHub Actions指南](https://docs.github.com/en/actions) | 自动化流水线 |
| 安全    | [Trivy](https://aquasecurity.github.io/trivy/)         | 镜像扫描   |
| 测试    | [Locust](https://locust.io/)                           | 负载测试   |

### 🎯 阶段成果

- **生产系统**：K8s部署的全功能应用
- **自动化流水线**：代码提交→测试→构建→部署
- **产品文档**：完整技术文档与API参考
- **演示资源**：产品视频+压力测试报告+架构图

---

## 📊 学习进度跟踪

### 🎯 每周复盘模板

```markdown
📅 第X周学习复盘（Day X-X）

## 🎯 本周目标完成情况

- [ ] Python任务：完成情况
- [ ] Java任务：完成情况
- [ ] TypeScript任务：完成情况
- [ ] 集成任务：完成情况

## 🧠 学习收获

- **Python层**：
- **Java层**：
- **TypeScript层**：
- **系统集成**：

## 💡 遇到的问题与解决方案

- 问题1（Python）：描述 + 解决方案
- 问题2（Java）：描述 + 解决方案
- 问题3（集成）：描述 + 解决方案

## 📈 下周计划

- **Python重点**：
- **Java重点**：
- **TypeScript重点**：
- **集成重点**：

## 🔄 计划调整

- 是否需要调整学习节奏？
- 是否需要补充特定技术点？
- 是否需要简化/强化某部分？
```

### 📋 阶段验收标准

| 阶段   | Python层            | Java层           | TypeScript层 | 集成成果      |
|------|--------------------|-----------------|-------------|-----------|
| 第1阶段 | FastAPI聊天接口        | Spring Boot服务代理 | 基础聊天界面      | 三层简单集成    |
| 第2阶段 | vLLM部署+指标暴露        | 服务发现+负载均衡       | SSE流式响应     | 可观测的推理服务  |
| 第3阶段 | RAG-Fusion+Ragas评估 | 文件管理+异步任务       | 文档上传+问答界面   | 企业级知识库系统  |
| 第4阶段 | LangGraph+工具开发     | 任务状态+Webhook    | 任务看板+审核界面   | 可审计的智能体系统 |
| 第5阶段 | QLoRA微调+AWQ量化      | 全链路监控+降级策略      | 监控面板        | 优化后的生产系统  |
| 第6阶段 | LLM服务Helm Chart    | 用户系统+计费         | 产品级UI       | 完整产品交付    |

---

## 🎯 最终成果清单

### 📚 技术文档

- [ ] **系统架构文档**：三层架构设计、数据流、部署拓扑
- [ ] **API文档**：Swagger UI + Postman集合
- [ ] **部署指南**：本地/Docker/K8s三种部署方式
- [ ] **优化报告**：微调/量化/缓存策略效果对比

### 🛠️ 代码仓库

- [ ] **Python仓库**：LLM服务、RAG、Agent、微调脚本
    - 目录结构：`/api`, `/rag`, `/agent`, `/training`, `/docker`
- [ ] **Java仓库**：Spring Boot后端
    - 目录结构：`/llm-proxy`, `/document`, `/agent-task`, `/user`, `/monitoring`
- [ ] **TypeScript仓库**：Next.js前端
    - 目录结构：`/app/(chat)`, `/app/(documents)`, `/app/(tasks)`, `/components`, `/lib`

### 📊 运维资产

- [ ] **Helm Chart**：K8s部署模板
- [ ] **Grafana仪表盘**：JSON导出
- [ ] **压力测试报告**：Locust结果
- [ ] **安全扫描报告**：Trivy+OWASP结果

### 🎥 产品资料

- [ ] **3分钟演示视频**：核心功能展示
- [ ] **架构图**：Mermaid/Draw.io格式
- [ ] **成本分析**：优化前后对比
- [ ] **用户手册**：快速入门指南

---

## 💡 学习建议

### 🔑 三层架构最佳实践

1. **明确边界**：
    - Python层：只处理LLM相关逻辑（不包含业务规则）
    - Java层：专注业务逻辑与系统集成（不包含模型细节）
    - TypeScript层：关注用户体验（不包含复杂业务规则）

2. **接口设计**：
    - 定义清晰的API契约（OpenAPI/Swagger）
    - 使用JSON Schema验证所有输入/输出
    - 版本化API（/v1/chat）

3. **调试策略**：
    - Python：LangSmith追踪
    - Java：Spring Boot Actuator + Zipkin
    - TypeScript：React DevTools + console.log

### ⚠️ 常见陷阱规避

```diff
- 不要混合技术栈职责：避免在Java中写模型推理逻辑，或在Python中写复杂业务规则
- 不要忽视可观测性：从第一天开始集成监控，而非项目结束
+ 必须设计降级策略：LLM服务不可用时，系统应优雅降级
+ 必须控制成本：设置Token用量阈值，避免意外高额账单
```

### 🚀 成功公式

**生产级LLM应用 = (Python模型能力 × Java工程化) + TypeScript用户体验**  
**核心原则：各司其职，松耦合集成，关注端到端体验**

> **行动起点：**
> 1. 用Python写一个FastAPI `/chat` 接口（D1）
> 2. 用Java Spring Boot调用它（D2）
> 3. 用TypeScript展示结果（D3）  
     > **你已迈出全栈LLM工程师的第一步！** 💪